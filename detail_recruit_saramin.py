import sys
import requests
from bs4 import BeautifulSoup
import json

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "Referer": "https://www.saramin.co.kr",
}


def fetch_detail_html(url: str) -> str:
    resp = requests.get(url, headers=HEADERS, timeout=10)
    resp.raise_for_status()
    return resp.text


def _extract_section_by_keyword(soup: BeautifulSoup, keywords):
    """
    'ìê²©ìš”ê±´', 'ìš°ëŒ€ì‚¬í•­' ê°™ì€ í‚¤ì›Œë“œê°€ ë“¤ì–´ê°„ ì œëª© ê·¼ì²˜ì—ì„œ
    <ul><li> ëª©ë¡ì„ ì°¾ì•„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜.
    """
    if isinstance(keywords, str):
        keywords = [keywords]

    target_heading = None

    def match_tag(tag):
        if tag.name not in ["h2", "h3", "h4", "strong", "span", "p"]:
            return False
        text = tag.get_text(strip=True)
        if not text:
            return False
        return any(k in text for k in keywords)

    target_heading = soup.find(match_tag)
    if not target_heading:
        return []

    # ìš°ì„  ê°™ì€ ë¸”ë¡ ì•ˆì˜ ul, ì—†ìœ¼ë©´ ë‹¤ìŒ ul
    container = target_heading.parent
    ul = container.find("ul")
    if not ul:
        ul = target_heading.find_next("ul")

    items = []
    if ul:
        for li in ul.find_all("li"):
            txt = li.get_text(" ", strip=True)
            if txt:
                items.append(txt)
    return items


def parse_detail(html: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")

    # ì œëª© / íšŒì‚¬ëª… (relay view ìƒë‹¨ ì˜ì—­ ê¸°ì¤€, ì•ˆ ë§ìœ¼ë©´ í´ë˜ìŠ¤ ìˆ˜ì • í•„ìš”)
    title_tag = soup.find("h1", class_="title") or soup.find("h1")
    title = title_tag.get_text(strip=True) if title_tag else None

    company_tag = soup.find("a", class_="company") or soup.find("strong", class_="corp_name")
    company = company_tag.get_text(strip=True) if company_tag else None

    # ìê²©ìš”ê±´ / ìš°ëŒ€ì‚¬í•­ / ë‹´ë‹¹ì—…ë¬´ ë“±
    required_conds = _extract_section_by_keyword(soup, ["ìê²©ìš”ê±´", "ì§€ì›ìê²©"])
    preferred_conds = _extract_section_by_keyword(soup, ["ìš°ëŒ€ì‚¬í•­", "ìš°ëŒ€ì¡°ê±´"])
    duties = _extract_section_by_keyword(soup, ["ë‹´ë‹¹ì—…ë¬´", "ì£¼ìš”ì—…ë¬´"])

    # ëª¨ì§‘/ì ‘ìˆ˜ ê¸°ê°„ í…ìŠ¤íŠ¸ (ë¸”ë¡ ì „ì²´ë¥¼ í•œ ì¤„ë¡œ ê°€ì ¸ì˜¤ëŠ” ì‹)
    period_tag = soup.find(
        lambda tag: tag.name in ["li", "p", "span", "div"]
        and tag.get_text(strip=True)
        and ("ëª¨ì§‘ê¸°ê°„" in tag.get_text() or "ì ‘ìˆ˜ê¸°ê°„" in tag.get_text())
    )
    period_text = period_tag.get_text(" ", strip=True) if period_tag else None

    # ê¸°íƒ€ ê·¼ë¬´ì¡°ê±´ ë¹„ìŠ·í•œ ë¸”ë¡ë„ í•„ìš”í•˜ë©´ ë¹„ìŠ·í•œ ë°©ì‹ìœ¼ë¡œ ì¶”ê°€ ê°€ëŠ¥
    # ex) _extract_section_by_keyword(soup, ["ê¸‰ì—¬", "ê·¼ë¬´ì¡°ê±´"])

    return {
        "title": title,
        "company": company,
        "required_conditions": required_conds,
        "preferred_conditions": preferred_conds,
        "duties": duties,
        "period": period_text,
    }


def main():
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python detail_recruit_saramin.py <ê³µê³  URL>")
        sys.exit(1)

    url = sys.argv[1]
    print(f"ğŸ” ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§: {url}")

    html = fetch_detail_html(url)
    data = parse_detail(html)

    # ì„œë²„ ì—°ë™ìš©ì´ë¼ë©´ JSONìœ¼ë¡œ ì§ë ¬í™”í•´ì„œ ë°˜í™˜í•˜ëŠ” í˜•íƒœë¥¼ ë§ì¶°ë‘ë©´ ì¢‹ìŒ
    print(json.dumps(data, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    main()
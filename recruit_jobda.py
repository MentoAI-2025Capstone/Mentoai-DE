import json  # JSON ì €ì¥ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import time
from urllib.parse import urljoin

BASE_URL = "https://www.jobda.im"
POSITION_URL = "https://www.jobda.im/position"

def fetch_jobda_html_with_scroll():
    """ìŠ¤í¬ë¡¤ì„ ëê¹Œì§€ ë‚´ë ¤ ëª¨ë“  ê³µê³ ë¥¼ ë¡œë”©í•œ í›„ HTMLì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    
    options = webdriver.ChromeOptions()
    options.add_argument('headless') 
    options.add_argument('window-size=1920x1080')
    options.add_argument("disable-gpu") 
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    
    try:
        print("ğŸŒ ì¡ë‹¤(Jobda) ì‚¬ì´íŠ¸ ì ‘ì† ì¤‘...")
        driver.get(POSITION_URL)
        time.sleep(3) # ì´ˆê¸° ë¡œë”© ëŒ€ê¸°

        # --- [í•µì‹¬] ë¬´í•œ ìŠ¤í¬ë¡¤ ë¡œì§ ---
        last_height = driver.execute_script("return document.body.scrollHeight")
        
        while True:
            # 1. ìŠ¤í¬ë¡¤ì„ ë§¨ ì•„ë˜ë¡œ ë‚´ë¦¼
            print("â¬‡ï¸ ìŠ¤í¬ë¡¤ì„ ë‚´ë¦¬ëŠ” ì¤‘...")
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            
            # 2. ë°ì´í„° ë¡œë”© ëŒ€ê¸° (ì¸í„°ë„· ì†ë„ì— ë”°ë¼ ì¡°ì ˆ í•„ìš”, 2~3ì´ˆ ê¶Œì¥)
            time.sleep(2)
            
            # 3. ìŠ¤í¬ë¡¤ í›„ ë†’ì´ ë¹„êµ (ë” ì´ìƒ ë‚´ë ¤ê°ˆ ê³³ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                print("âœ… ìŠ¤í¬ë¡¤ ì™„ë£Œ! ë” ì´ìƒ ë¶ˆëŸ¬ì˜¬ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
                break
            last_height = new_height
        # -----------------------------
        
        html = driver.page_source
        return html
    except Exception as e:
        print(f"âŒ Selenium ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None
    finally:
        driver.quit()

def parse_jobda_detail(html: str):
    """HTMLì—ì„œ ê³µê³  ì •ë³´ë¥¼ ìƒì„¸ ë¶„ë¦¬í•˜ì—¬ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    soup = BeautifulSoup(html, "html.parser")
    items = []

    all_links = soup.find_all("a", href=True)
    job_links = [a for a in all_links if "/position/" in a["href"] and len(a["href"]) > 10]
    
    # ì¤‘ë³µ ì œê±° (ê°€ë” ìŠ¤í¬ë¡¤ ê³¼ì •ì—ì„œ ì¤‘ë³µëœ ìš”ì†Œê°€ ì¡í ìˆ˜ ìˆìŒ)
    # ë§í¬(href)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µì„ ì œê±°í•˜ê¸° ìœ„í•´ ë”•ì…”ë„ˆë¦¬ ì‚¬ìš© í›„ ë¦¬ìŠ¤íŠ¸ ë³€í™˜ ê³ ë ¤ ê°€ëŠ¥í•˜ì§€ë§Œ,
    # ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ ë¦¬ìŠ¤íŠ¸ë¡œ ë‹¤ ë°›ìŠµë‹ˆë‹¤.
    
    print(f"ğŸ” ë°œê²¬ëœ ê³µê³  ìš”ì†Œ ê°œìˆ˜: {len(job_links)}ê°œ")

    for item in job_links:
        link = urljoin(BASE_URL, item.get("href"))
        
        img_tag = item.find("img")
        img_url = img_tag["src"] if img_tag else ""
        
        text_parts = list(item.stripped_strings)
        title = text_parts[0] if len(text_parts) > 0 else ""
        company = text_parts[1] if len(text_parts) > 1 else ""
        etc_info = " / ".join(text_parts[2:]) if len(text_parts) > 2 else ""

        items.append({
            "title": title,
            "company": company,
            "etc": etc_info,
            "img_url": img_url,
            "link": link
        })

    return items

if __name__ == "__main__":
    # 1. ìŠ¤í¬ë¡¤ê¹Œì§€ í¬í•¨í•´ì„œ HTML ê°€ì ¸ì˜¤ê¸°
    html = fetch_jobda_html_with_scroll()
    
    if html:
        # 2. íŒŒì‹±
        job_postings = parse_jobda_detail(html)

        print("-" * 50)
        print(f"ğŸ“Š ìµœì¢… ìˆ˜ì§‘ëœ ê³µê³  ê°œìˆ˜: {len(job_postings)}ê°œ")
        print("-" * 50)
        
        # 3. JSON íŒŒì¼ë¡œ ì €ì¥
        json_filename = "jobda_result.json"
        
        try:
            with open(json_filename, "w", encoding="utf-8") as f:
                # ensure_ascii=False : í•œê¸€ì´ ê¹¨ì§€ì§€ ì•Šê³  ë³´ì´ê²Œ í•¨
                # indent=4 : ë“¤ì—¬ì“°ê¸°ë¥¼ í•´ì„œ ë³´ê¸° ì¢‹ê²Œ ì €ì¥í•¨
                json.dump(job_postings, f, ensure_ascii=False, indent=4)
                
            print(f"ğŸ’¾ '{json_filename}' íŒŒì¼ë¡œ ì˜ˆì˜ê²Œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
            
        except Exception as e:
            print(f"âŒ JSON ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
    else:
        print("âŒ HTMLì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")